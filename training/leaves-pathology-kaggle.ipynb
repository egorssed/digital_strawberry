{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{"id":"WbY308Alxq_W"}},{"cell_type":"markdown","source":"<center><img src=\"https://i.imgur.com/vSUSbDf.jpg\" width=\"500px\"></center>","metadata":{}},{"cell_type":"markdown","source":"Welcome to the \"Plant Pathology 2020 - FGVC7\" competition! In this competition, contestants are challenged to diagnose plant diseases solely based on leaf images. The categories include \"healthy\", \"scab\", \"rust\", and \"multiple diseases\". Solving this problem is important because diagnosing plant diseases early can save tonnes of agricultural produce every year. This will benefit not only the general population by reducing hunger, but also the farmers by ensuring they get the harvest they deserve.\n\nIn this kernel, I will visualize the data with Matplotlib and Plotly and then demonstrate some important image processing and augmentation techniques using OpenCV. Finally, I will show how different pretrained Keras models, such as DenseNet and EfficientNet, can be used to solve the problem.\n\n<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)</font>","metadata":{}},{"cell_type":"markdown","source":"To get started, here is an excellent video about how data scientists use TensorFlow to detect diseases in Cassava plants in Africa:","metadata":{"id":"1f3iYUa60vaG"}},{"cell_type":"markdown","source":"## Preparing the ground <a id=\"1.1\"></a>","metadata":{"id":"ufcI1cah2o_T"}},{"cell_type":"markdown","source":"### Install and import necessary libraries","metadata":{"id":"1XvcaAzr2rjY"}},{"cell_type":"code","source":"!pip install -q efficientnet","metadata":{"id":"V8TgyWRwpnKH","execution":{"iopub.status.busy":"2021-11-20T20:24:00.004958Z","iopub.execute_input":"2021-11-20T20:24:00.005416Z","iopub.status.idle":"2021-11-20T20:24:10.281296Z","shell.execute_reply.started":"2021-11-20T20:24:00.005304Z","shell.execute_reply":"2021-11-20T20:24:10.280056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\n\nimport cv2\nimport math\nimport pickle\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport tensorflow as tf\nfrom IPython.display import SVG\nimport efficientnet.tfkeras as efn\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.utils import model_to_dot\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.applications import DenseNet121\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\ntqdm.pandas()\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nnp.random.seed(0)\ntf.random.set_seed(0)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef display_training_curves(training, validation, yaxis):\n    if yaxis == \"loss\":\n        ylabel = \"Loss\"\n        title = \"Loss vs. Epochs\"\n    else:\n        ylabel = \"Accuracy\"\n        title = \"Accuracy vs. Epochs\"\n        \n    fig = go.Figure()\n        \n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=training, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"))\n\n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=validation, marker=dict(color=\"darkorange\"),\n               name=\"Val\"))\n\n    fig.update_layout(title_text=title, yaxis_title=ylabel, xaxis_title=\"Epochs\", template=\"plotly_white\")\n    fig.show()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"y4ElXcLopnKO","outputId":"72756d56-48f2-46c7-d8d0-56ec9fc395ce","execution":{"iopub.status.busy":"2021-11-20T20:24:10.28354Z","iopub.execute_input":"2021-11-20T20:24:10.283914Z","iopub.status.idle":"2021-11-20T20:24:19.938451Z","shell.execute_reply.started":"2021-11-20T20:24:10.283877Z","shell.execute_reply":"2021-11-20T20:24:19.937416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the data and define hyperparameters","metadata":{"id":"DyZKLcDg2yRi"}},{"cell_type":"code","source":"EPOCHS = 20\nSAMPLE_LEN = 100\nIMAGE_PATH = \"../input/plant-pathology-2020-fgvc7/images/\"\nTEST_PATH = \"../input/plant-pathology-2020-fgvc7/test.csv\"\nTRAIN_PATH = \"../input/plant-pathology-2020-fgvc7/train.csv\"\nSUB_PATH = \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"\n\nsub = pd.read_csv(SUB_PATH)\ntest_data = pd.read_csv(TEST_PATH)\ntrain_data = pd.read_csv(TRAIN_PATH)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"mneU8D9bpnKS","execution":{"iopub.status.busy":"2021-11-20T20:24:19.93986Z","iopub.execute_input":"2021-11-20T20:24:19.940113Z","iopub.status.idle":"2021-11-20T20:24:19.983562Z","shell.execute_reply.started":"2021-11-20T20:24:19.940084Z","shell.execute_reply":"2021-11-20T20:24:19.982823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"### Setup TPU Config","metadata":{"id":"zOfbl73V6t3p"}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","metadata":{"id":"2ZC6VPQHpnMR","execution":{"iopub.status.busy":"2021-11-20T20:24:19.986135Z","iopub.execute_input":"2021-11-20T20:24:19.986481Z","iopub.status.idle":"2021-11-20T20:24:26.125946Z","shell.execute_reply.started":"2021-11-20T20:24:19.986433Z","shell.execute_reply":"2021-11-20T20:24:26.124913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load labels and paths","metadata":{"id":"SuAHc2hu6-Nu"}},{"cell_type":"code","source":"def format_path(st):\n    return GCS_DS_PATH + '/images/' + st + '.jpg'\n\ntest_paths = test_data.image_id.apply(format_path).values\ntrain_paths = train_data.image_id.apply(format_path).values\n\ntrain_labels = np.float32(train_data.loc[:, 'healthy':'scab'].values)\ntrain_paths, valid_paths, train_labels, valid_labels =\\\ntrain_test_split(train_paths, train_labels, test_size=0.15, random_state=2020)","metadata":{"id":"9BALmDtRpnMU","execution":{"iopub.status.busy":"2021-11-20T20:24:26.127397Z","iopub.execute_input":"2021-11-20T20:24:26.127631Z","iopub.status.idle":"2021-11-20T20:24:26.156035Z","shell.execute_reply.started":"2021-11-20T20:24:26.127604Z","shell.execute_reply":"2021-11-20T20:24:26.155052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.loc[:, 'healthy':'scab'].columns","metadata":{"execution":{"iopub.status.busy":"2021-11-20T20:26:13.663492Z","iopub.execute_input":"2021-11-20T20:26:13.66381Z","iopub.status.idle":"2021-11-20T20:26:13.670264Z","shell.execute_reply.started":"2021-11-20T20:26:13.663777Z","shell.execute_reply":"2021-11-20T20:26:13.669735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels","metadata":{"execution":{"iopub.status.busy":"2021-11-20T20:24:56.455304Z","iopub.execute_input":"2021-11-20T20:24:56.455613Z","iopub.status.idle":"2021-11-20T20:24:56.466122Z","shell.execute_reply.started":"2021-11-20T20:24:56.455582Z","shell.execute_reply":"2021-11-20T20:24:56.465219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(512, 512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n\n    if label is None:\n        return image\n    else:\n        return image, label","metadata":{"id":"T84Nnc1jpnMW","execution":{"iopub.status.busy":"2021-11-20T13:51:55.786748Z","iopub.execute_input":"2021-11-20T13:51:55.787102Z","iopub.status.idle":"2021-11-20T13:51:55.806593Z","shell.execute_reply.started":"2021-11-20T13:51:55.787058Z","shell.execute_reply":"2021-11-20T13:51:55.805783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Dataset objects","metadata":{"id":"tonEhhQ77Knh"}},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","metadata":{"id":"5rkIRCnupnMZ","execution":{"iopub.status.busy":"2021-11-20T13:51:55.807552Z","iopub.execute_input":"2021-11-20T13:51:55.807769Z","iopub.status.idle":"2021-11-20T13:51:56.069009Z","shell.execute_reply.started":"2021-11-20T13:51:55.807745Z","shell.execute_reply":"2021-11-20T13:51:56.068095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper functions","metadata":{"id":"xmirtR2L7TDC"}},{"cell_type":"code","source":"def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","metadata":{"id":"uiiCB9SdpnMc","execution":{"iopub.status.busy":"2021-11-19T23:12:44.728315Z","iopub.execute_input":"2021-11-19T23:12:44.728578Z","iopub.status.idle":"2021-11-19T23:12:44.735938Z","shell.execute_reply.started":"2021-11-19T23:12:44.728548Z","shell.execute_reply":"2021-11-19T23:12:44.735145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define hyperparameters and callbacks","metadata":{}},{"cell_type":"code","source":"lrfn = build_lrfn()\nSTEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T23:12:44.737542Z","iopub.execute_input":"2021-11-19T23:12:44.737808Z","iopub.status.idle":"2021-11-19T23:12:44.749176Z","shell.execute_reply.started":"2021-11-19T23:12:44.737778Z","shell.execute_reply":"2021-11-19T23:12:44.748381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EfficientNet NoisyStudent <a id=\"3.4\"></a>\n\nEfficientNet NoisyStudent, released in 2020, is based on EfficientNet and uses semi-supervised learning on noisy images to learn rich visual representation. It outperformed EfficientNet on several tasks and is the SOTA at the time of writing (March 2020). Now let us train EfficientNet NoisyStudent on leaf images and evaluate its performance.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([efn.EfficientNetB7(input_shape=(512, 512, 3),\n                                                    weights='noisy-student',\n                                                    include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n\n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T23:12:44.768107Z","iopub.execute_input":"2021-11-19T23:12:44.76838Z","iopub.status.idle":"2021-11-19T23:13:46.428769Z","shell.execute_reply.started":"2021-11-19T23:12:44.768344Z","shell.execute_reply":"2021-11-19T23:13:46.427764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EfficientNet NoisyStudent","metadata":{}},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-20T00:07:58.129652Z","iopub.execute_input":"2021-11-20T00:07:58.130018Z","iopub.status.idle":"2021-11-20T00:07:58.579689Z","shell.execute_reply.started":"2021-11-20T00:07:58.129979Z","shell.execute_reply":"2021-11-20T00:07:58.577368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above image shows the fundamental block in the EfficientNet NoisyStudent architecture. This model has the same architecture as EfficientNet. Only the weights are different, as they are obtained through semi-supervision.","metadata":{}},{"cell_type":"markdown","source":"### Visualize model architecture\n\nThe model consists of the EfficientNet NoisyStudent head (without the top), followed by global average pooling and a dense layer (with softmax) to generate probabilities.","metadata":{}},{"cell_type":"code","source":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-20T00:07:59.038671Z","iopub.execute_input":"2021-11-20T00:07:59.039423Z","iopub.status.idle":"2021-11-20T00:07:59.426619Z","shell.execute_reply.started":"2021-11-20T00:07:59.039378Z","shell.execute_reply":"2021-11-20T00:07:59.425657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train model","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-19T23:13:47.543668Z","iopub.execute_input":"2021-11-19T23:13:47.54441Z","iopub.status.idle":"2021-11-19T23:30:26.103051Z","shell.execute_reply.started":"2021-11-19T23:13:47.544367Z","shell.execute_reply":"2021-11-19T23:30:26.102182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save Model","metadata":{}},{"cell_type":"code","source":"!ls -la en_v1_weights","metadata":{"execution":{"iopub.status.busy":"2021-11-20T00:06:05.623636Z","iopub.execute_input":"2021-11-20T00:06:05.623968Z","iopub.status.idle":"2021-11-20T00:06:06.574477Z","shell.execute_reply.started":"2021-11-20T00:06:05.623933Z","shell.execute_reply":"2021-11-20T00:06:06.573429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('en_v1_weights/chekpoint.h5')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T00:05:47.3201Z","iopub.execute_input":"2021-11-20T00:05:47.320468Z","iopub.status.idle":"2021-11-20T00:05:52.252718Z","shell.execute_reply.started":"2021-11-20T00:05:47.320434Z","shell.execute_reply":"2021-11-20T00:05:52.251874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scatter plots","metadata":{}},{"cell_type":"code","source":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-19T20:59:12.821132Z","iopub.execute_input":"2021-11-19T20:59:12.822Z","iopub.status.idle":"2021-11-19T20:59:14.194436Z","shell.execute_reply.started":"2021-11-19T20:59:12.821937Z","shell.execute_reply":"2021-11-19T20:59:14.192875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we can see that the losses decrease and accuracies increase quite consistently. The training metrics settle down very fast (after 1 or 2 epochs), whereas the validation metrics much greater volatility and start to settle down only after 12-13 epochs (similar to DenseNet). This is expected because validation data is unseen and more diffcult to make predictions on than training data. ","metadata":{}},{"cell_type":"markdown","source":"### Animation (click ▶️)","metadata":{}},{"cell_type":"code","source":"acc_df = pd.DataFrame(np.transpose([[*np.arange(1, EPOCHS+1).tolist()*3], [\"Train\"]*EPOCHS + [\"Val\"]*EPOCHS + [\"Benchmark\"]*EPOCHS,\n                                     history.history['categorical_accuracy'] + history.history['val_categorical_accuracy'] + [1.0]*EPOCHS]))\nacc_df.columns = [\"Epochs\", \"Stage\", \"Accuracy\"]\nfig = px.bar(acc_df, x=\"Accuracy\", y=\"Stage\", animation_frame=\"Epochs\", title=\"Accuracy vs. Epochs\", color='Stage',\n       color_discrete_map={\"Train\":\"dodgerblue\", \"Val\":\"darkorange\", \"Benchmark\":\"seagreen\"}, orientation=\"h\")\n\nfig.update_layout(\n    xaxis = dict(\n        autorange=False,\n        range=[0, 1]\n    )\n)\n\nfig.update_layout(template=\"plotly_white\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-19T21:02:47.931676Z","iopub.execute_input":"2021-11-19T21:02:47.932382Z","iopub.status.idle":"2021-11-19T21:02:48.82708Z","shell.execute_reply.started":"2021-11-19T21:02:47.932336Z","shell.execute_reply":"2021-11-19T21:02:48.82577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the animations above, we can see the volatility in validation metrics a lot more clearly. The validation metrics oscillate in an erratic fashion until it reaches the 12th epoch and starts to generalize properly.","metadata":{}},{"cell_type":"markdown","source":"### Sample predictions\n\nNow, I will visualize some sample predictions made by the EfficientNet NoisyStudent model. The <font color=\"red\">red</font> bars represent the model's prediction (maximum probability), the <font color=\"green\">green</font> represent the ground truth (label), and the rest of the bars are <font color=\"blue\">blue</font>. When the model predicts correctly, the prediction bar is <font color=\"green\">green</font>.","metadata":{}},{"cell_type":"code","source":"def load_image(file_path):\n#     file_path = image_id + \".jpg\"\n    image = cv2.imread(IMAGE_PATH + file_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\ntrain_images = pd.Series([p.split('/')[-1] for p in valid_paths]).progress_apply(load_image) #train_data[\"image_id\"][:SAMPLE_LEN].progress_apply(load_image)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T00:12:24.982569Z","iopub.execute_input":"2021-11-20T00:12:24.982892Z","iopub.status.idle":"2021-11-20T00:12:39.147953Z","shell.execute_reply.started":"2021-11-20T00:12:24.982858Z","shell.execute_reply":"2021-11-20T00:12:39.147057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process(img):\n    return cv2.resize(img/255.0, (512, 512)).reshape(-1, 512, 512, 3)\ndef predict(img):\n    return model.layers[2](model.layers[1](model.layers[0](process(img)))).numpy()[0]\n\nfig = make_subplots(rows=4, cols=2)\npreds = predict(train_images[2])\n\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Scab\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Multiple diseases\"\n\ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Healthy\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[2], (205, 136))), row=1, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=1, col=2)\nfig.update_layout(height=1200, width=800, title_text=\"EfficientNet NoisyStudent Predictions\", showlegend=False)\n\npreds = predict(train_images[0])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Multiple diseases\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[0], (205, 136))), row=2, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=2, col=2)\n\npreds = predict(train_images[3])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Rust\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[3], (205, 136))), row=3, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=3, col=2)\n\npreds = predict(train_images[1])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Scab\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[1], (205, 136))), row=4, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=4, col=2)\nfig.update_layout(template=\"plotly_white\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-19T21:16:12.397571Z","iopub.execute_input":"2021-11-19T21:16:12.397981Z","iopub.status.idle":"2021-11-19T21:16:28.813053Z","shell.execute_reply.started":"2021-11-19T21:16:12.397948Z","shell.execute_reply":"2021-11-19T21:16:28.812073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images[1].shape","metadata":{"execution":{"iopub.status.busy":"2021-11-20T00:12:39.149235Z","iopub.execute_input":"2021-11-20T00:12:39.149488Z","iopub.status.idle":"2021-11-20T00:12:39.157491Z","shell.execute_reply.started":"2021-11-20T00:12:39.149461Z","shell.execute_reply":"2021-11-20T00:12:39.156605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar to the DenseNet model, EfficientNet NoisyStudent predicts leaf diseases with great accuracy. No red bars are seen. The probabilities are very polarized (one very high and the rest very low), indicating that the model is making these predictions with great confidence. The semi-supervised weights seem to set this model apart from EfficientNet. The red and blue bars are, once again, more prominent in the last (fourth) leaf labeled \"multiple_diseases\". This is probably because leaves with multiple diseases may show symptoms of rust and scab as well, thus slightly confusing the model.","metadata":{}},{"cell_type":"markdown","source":"# Takeaways <a id=\"4\"></a>\n\n1. Image processing and augmentation methods such as edge detection, depth estimation, flipping, etc can be used to build  models.\n\n2. Several pretrained models like DenseNet and EfficientNet can be used to classify leaf diseases with high accuracy.\n\n3. Ensembling, stacking, and strong validation techniques may lead to more accurate and robust models.","metadata":{}},{"cell_type":"markdown","source":"# Ending note <a id=\"5\"></a>\n\n<font color=\"red\" size=4>This concludes my kernel. Please upvote if you like it. It motivates me to produce more quality content :)</font>","metadata":{}}]}